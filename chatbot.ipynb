{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIvN5v5WBsPwkvQjVbV3kv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sj11156/CHATBOT/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbx0Bxwfq6k3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies for Ollama (optional, but good for some systems)\n",
        "!sudo apt update && sudo apt install -y pciutils lshw\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start the Ollama server in the background\n",
        "# nohup ensures it keeps running even if the cell finishes\n",
        "# > ollama.log 2>&1 redirects output to a log file\n",
        "# & runs it in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Give Ollama a moment to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"Ollama server should be running.\")"
      ],
      "metadata": {
        "id": "_HVZJKXMrYnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Ollama server in the background\n",
        "# nohup ensures it keeps running even if the cell finishes\n",
        "# > ollama.log 2>&1 redirects output to a log file (good for debugging)\n",
        "# & runs it in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Give Ollama a moment to start up\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"Ollama server should now be running in the background.\")\n",
        "# You can optionally check the log for any initial errors\n",
        "# !cat ollama.log\n"
      ],
      "metadata": {
        "id": "1JZ2mXQftoCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the desired model (e.g., llama3)\n",
        "# This will download the model weights. It might take several minutes depending on the model size and your connection.\n",
        "!ollama pull llama3\n",
        "print(f\"Model llama3 pulled and ready.\")\n",
        "\n",
        "# You can verify the model is downloaded\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "qhqEqtn_t4vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-ollama streamlit"
      ],
      "metadata": {
        "id": "FFg193p7uy-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# --- Configuration ---\n",
        "# CHANGE 1: Use the multimodal model LLaVA\n",
        "MODEL_NAME = \"llava\"\n",
        "MAX_HISTORY = 5\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "# --- LangChain Components ---\n",
        "llm = ChatOllama(model=MODEL_NAME)\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    k=MAX_HISTORY\n",
        ")\n",
        "\n",
        "# 3. Create a Prompt Template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant. You can analyze images and reply based on provided context.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Create the Chain\n",
        "conversation_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "\n",
        "st.set_page_config(page_title=\"Ollama Multimodal Chatbot\", layout=\"centered\")\n",
        "st.title(\"Local LLM Chatbot (Vision Capable)\")\n",
        "\n",
        "# Initialize chat history in Streamlit session state\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        # CHANGE 2: Display image if present in the message\n",
        "        if \"content\" in message:\n",
        "            st.markdown(message[\"content\"])\n",
        "        if \"image\" in message:\n",
        "            st.image(message[\"image\"], width=200)\n",
        "\n",
        "# --------------------------\n",
        "# Image Upload Section (CHANGE 3)\n",
        "# --------------------------\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload an image (optional)\", type=['png', 'jpg', 'jpeg'])\n",
        "\n",
        "# React to user input\n",
        "if prompt_input := st.chat_input(\"Ask me anything or about the image...\"):\n",
        "\n",
        "    # Store the user's text input\n",
        "    user_message_content = prompt_input\n",
        "\n",
        "    # Define the input structure for LangChain's LLMChain.invoke()\n",
        "    langchain_input = {\"question\": user_message_content}\n",
        "\n",
        "    # --------------------------\n",
        "    # 4. Image Handling and Base64 Encoding\n",
        "    # --------------------------\n",
        "    if uploaded_file is not None:\n",
        "        # Read the image file content\n",
        "        image_bytes = uploaded_file.read()\n",
        "\n",
        "        # Convert image bytes to base64 format (required by Ollama for image input)\n",
        "        base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
        "\n",
        "        # Format the input for the multimodal model:\n",
        "        # LangChain's Ollama integration expects a list of dictionaries with type 'text' or 'image_url'\n",
        "\n",
        "        # This structure goes into the \"question\" variable of the LLMChain input\n",
        "        langchain_input[\"question\"] = [\n",
        "            {\"type\": \"text\", \"text\": user_message_content},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "        ]\n",
        "\n",
        "        # Add the image path to the Streamlit session state for display in the chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_message_content, \"image\": image_bytes})\n",
        "    else:\n",
        "        # If no image uploaded, just add the text content to the Streamlit session state\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_message_content})\n",
        "\n",
        "    # Display the user message in the chat\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_message_content)\n",
        "        if uploaded_file is not None:\n",
        "            st.image(uploaded_file, width=200)\n",
        "\n",
        "    # --------------------------\n",
        "    # 5. Invoke the LLM\n",
        "    # --------------------------\n",
        "\n",
        "    # Get response from the LLM chain\n",
        "    with st.spinner(\"Thinking...\"):\n",
        "        try:\n",
        "            # Invoke the chain with the current input (text and/or image)\n",
        "            # This works for both text-only and multimodal input\n",
        "            response = conversation_chain.invoke(langchain_input)\n",
        "            ai_response = response[\"text\"]\n",
        "        except Exception as e:\n",
        "            ai_response = f\"An error occurred: {e}\"\n",
        "            st.error(ai_response)\n",
        "\n",
        "    # --------------------------\n",
        "    # 6. Handle LLM Response\n",
        "    # --------------------------\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(ai_response)\n",
        "\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": ai_response})"
      ],
      "metadata": {
        "id": "eEpZa8uVv6ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false"
      ],
      "metadata": {
        "id": "j10RtCnrdxyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import os\n",
        "import time # Ensure time is imported\n",
        "\n",
        "# --- Important ngrok setup ---\n",
        "# 1. Get your ngrok Authtoken:\n",
        "#    Go to https://dashboard.ngrok.com/get-started/your-authtoken (you'll need to sign up)\n",
        "#    Copy your token and paste it below.\n",
        "#    This helps avoid rate limits and makes your tunnel more stable.\n",
        "\n",
        "# Kill any existing ngrok tunnels on port 8501 (Streamlit's default)\n",
        "!kill $(lsof -t -i:8501) 2>/dev/null || true\n",
        "\n",
        "# Define a function to start the ngrok tunnel\n",
        "def start_ngrok():\n",
        "    try:\n",
        "        # Connect to Streamlit's default port 8501\n",
        "        ngrok_tunnel = ngrok.connect(8501)\n",
        "        print(f\"Streamlit Public URL: {ngrok_tunnel.public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok: {e}\")\n",
        "        print(\"Make sure you've added your ngrok authtoken if you keep getting errors.\")\n",
        "        print(\"You can get one from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "\n",
        "# Start the ngrok tunnel in a separate thread\n",
        "ngrok_thread = threading.Thread(target=start_ngrok)\n",
        "ngrok_thread.setDaemon(True) # Ensure the thread stops when the main program stops\n",
        "ngrok_thread.start()\n",
        "\n",
        "# Give ngrok a moment to establish the tunnel\n",
        "time.sleep(5)\n",
        "\n",
        "# --- Run Streamlit App ---\n",
        "# This will run the app.py file we just created\n",
        "!streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false"
      ],
      "metadata": {
        "id": "5KiNxPwRxSLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add \"chatbot.ipynb\"\n",
        "!git commit -m \"Removed hardcoded API key and cleared outputs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVNf005HkwVJ",
        "outputId": "0ab67eb2-a5bf-47b6-b7c1-c3df7dc94869"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Ollama server in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Give Ollama a moment to start up\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"Ollama server should now be running in the background.\")"
      ],
      "metadata": {
        "id": "FoCo6tvGVMZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llava"
      ],
      "metadata": {
        "id": "p9hyMJtiVrjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llava"
      ],
      "metadata": {
        "id": "N88Buu3RWaW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "-pKkCadFRzWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-ollama streamlit pyngrok"
      ],
      "metadata": {
        "id": "OWT6yaI_PaTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_D1Iz1tPcyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw6yz4ttlgdE",
        "outputId": "596ef782-b833-4d60-90cb-7557222a0240"
      },
    
        
